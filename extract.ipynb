{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f50390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "from pathlib import Path\n",
    "import json, sqlite3, datetime, traceback, zipfile, tempfile, shutil\n",
    "\n",
    "# Finding repo root for group reproduction\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for p in [cur, *cur.parents]:\n",
    "        if (p / \".git\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "try:\n",
    "    REPO_ROOT = find_repo_root(Path(__file__).resolve().parent)\n",
    "except NameError:\n",
    "    REPO_ROOT = find_repo_root(Path.cwd())\n",
    "\n",
    "# Paths for dataset\n",
    "DATASET_NAME  = \"spider\"       # change for dataset name\n",
    "DATASET_DIR   = REPO_ROOT / \"data\" / DATASET_NAME\n",
    "DATASET_ZIP   = REPO_ROOT / \"data\" / f\"{DATASET_NAME}.zip\"\n",
    "\n",
    "RUN_STAMP      = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "ARTIFACTS_ROOT = REPO_ROOT / \"artifacts\" / \"runs\" / DATASET_NAME\n",
    "\n",
    "# Controls\n",
    "MAX_DATABASES      = 100   # 0/None for all\n",
    "ROWS_PER_TABLE     = 50\n",
    "SKIP_IF_EXISTS     = True  # set False to overwrite\n",
    "AUTO_EXTRACT       = True  # only used when EXTRACT_TO_TEMP is False\n",
    "EXTRACT_TO_TEMP    = True  # preferred: avoids leaving extracted files in repo\n",
    "CLEANUP_AFTER_RUN  = True\n",
    "\n",
    "TMP_DIR = None\n",
    "\n",
    "\n",
    "# Temporary extraction directory\n",
    "if EXTRACT_TO_TEMP and DATASET_ZIP.exists():\n",
    "    TMP_DIR = Path(tempfile.mkdtemp(prefix=f\"{DATASET_NAME}_\"))\n",
    "    \n",
    "    with zipfile.ZipFile(DATASET_ZIP, \"r\") as zf:\n",
    "        zf.extractall(TMP_DIR)\n",
    "    \n",
    "    # Point dataset dir at the temp extraction\n",
    "    DATASET_DIR = TMP_DIR / DATASET_NAME\n",
    "else:\n",
    "    # Only extract into repo if not using temp\n",
    "    if AUTO_EXTRACT and DATASET_ZIP.exists() and not DATASET_DIR.exists():\n",
    "        \n",
    "        with zipfile.ZipFile(DATASET_ZIP, \"r\") as zf:\n",
    "            zf.extractall(DATASET_DIR.parent)\n",
    "\n",
    "# Compute DB_ROOT AFTER extraction so we can detect a 'database' subfolder correctly\n",
    "DB_ROOT_CANDIDATE = DATASET_DIR / \"database\"\n",
    "DB_ROOT = DB_ROOT_CANDIDATE if DB_ROOT_CANDIDATE.exists() else DATASET_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17f22cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List tables in the database\n",
    "def list_tables(conn: sqlite3.Connection) -> list[str]:\n",
    "    cur = conn.execute(\n",
    "        \"SELECT name FROM sqlite_master \"\n",
    "        \"WHERE type='table' AND name NOT LIKE 'sqlite_%' ORDER BY name\"\n",
    "    )\n",
    "    return [r[0] for r in cur.fetchall()]\n",
    "\n",
    "# Get columns for a table \n",
    "def get_table_columns(conn: sqlite3.Connection, table: str) -> list[dict]:\n",
    "    # Returns list of column dicts: {\"name\", \"type\", \"notnull\", \"default\", \"is_pk\"}.\n",
    "    cols = []\n",
    "    for cid, name, ctype, notnull, dflt_value, pk in conn.execute(f\"PRAGMA table_info('{table}')\"):\n",
    "        cols.append({\n",
    "            \"name\": name,\n",
    "            \"type\": ctype,\n",
    "            \"notnull\": bool(notnull),\n",
    "            \"default\": dflt_value,\n",
    "            \"is_pk\": bool(pk),\n",
    "        })\n",
    "    return cols\n",
    "\n",
    "# find primary key columns\n",
    "def find_pk_columns(cols: list[dict]) -> list[str]:\n",
    "    pks = [c[\"name\"] for c in cols if c.get(\"is_pk\")]\n",
    "    return pks\n",
    "\n",
    "def get_foreign_keys(conn: sqlite3.Connection, table: str) -> list[dict]:\n",
    "    # Returns list of FKs as dicts with from_table, from_column, to_table, to_column.\n",
    "    fks = []\n",
    "    for row in conn.execute(f\"PRAGMA foreign_key_list('{table}')\"):\n",
    "        # columns based on SQLite docs: id, seq, table, from, to, on_update, on_delete, match\n",
    "        _, _, ref_table, from_col, to_col, *_ = row\n",
    "        fks.append({\n",
    "            \"from_table\": table,\n",
    "            \"from_column\": from_col,\n",
    "            \"to_table\": ref_table,\n",
    "            \"to_column\": to_col,\n",
    "        })\n",
    "    return fks\n",
    "\n",
    "# Sample rows from a table\n",
    "def sample_rows(conn: sqlite3.Connection, table: str, n: int) -> list[dict]:\n",
    "    cur = conn.execute(f\"SELECT * FROM '{table}' LIMIT {int(n)}\")\n",
    "    colnames = [d[0] for d in cur.description]\n",
    "    rows = []\n",
    "    for r in cur.fetchall():\n",
    "        rows.append({c: v for c, v in zip(colnames, r)})\n",
    "    return rows\n",
    "\n",
    "# Counts how many times each column has null values\n",
    "def summarize_nulls(rows: list[dict]) -> dict:\n",
    "    if not rows:\n",
    "        return {}\n",
    "    cols = rows[0].keys()\n",
    "    out = {c: 0 for c in cols}\n",
    "    for r in rows:\n",
    "        for c in cols:\n",
    "            if r.get(c) is None:\n",
    "                out[c] += 1\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dac9164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scans a dataset folder and finds every sqlite file giving them readable IDs with the exact path\n",
    "def enumerate_sqlite_dbs(db_root: Path) -> list[tuple[str, Path]]:\n",
    "    paths = sorted(db_root.rglob(\"*.sqlite\"))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No .sqlite files found under {db_root}\")\n",
    "    pairs = []\n",
    "    for p in paths:\n",
    "        db_id = p.parent.name or p.stem\n",
    "        pairs.append((db_id, p))\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main function that handles a single SQLite database end-to-end\n",
    "# This will extract the schema, sample data, and validation results from the database.\n",
    "\n",
    "def process_one_db(db_id: str, db_path: Path, out_root: Path) -> None:\n",
    "    # Create output directory for this database\n",
    "    OUT_DIR = out_root / db_id\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    \n",
    "    schema_path  = OUT_DIR / \"schema.json\"\n",
    "    samples_path = OUT_DIR / \"samples.jsonl\"\n",
    "    val_path     = OUT_DIR / \"validation.log\"\n",
    "    \n",
    "\n",
    "    # Skip processing if outputs already exist\n",
    "    if SKIP_IF_EXISTS and schema_path.exists() and samples_path.exists() and val_path.exists():\n",
    "        print(f\"Already exists, skipping: {OUT_DIR}\")\n",
    "        return\n",
    "\n",
    "    conn = None\n",
    "\n",
    "    try:\n",
    "        # Open DB in read-only mode\n",
    "        conn = sqlite3.connect(f\"file:{db_path}?mode=ro\", uri=True)\n",
    "        conn.row_factory = sqlite3.Row\n",
    "\n",
    "        # 1. Discover tables in the database\n",
    "        tables = list_tables(conn)\n",
    "\n",
    "        # 2. Build schema representation (tables, columns, PKs, FKs)\n",
    "        schema = {\"db_id\": db_id, \"tables\": [], \"foreign_keys\": []}\n",
    "        table_to_columns = {}\n",
    "        for t in tables:\n",
    "            cols = get_table_columns(conn, t)\n",
    "            table_to_columns[t] = cols\n",
    "            schema[\"tables\"].append({\n",
    "                \"name\": t,\n",
    "                \"columns\": cols,\n",
    "                \"primary_key\": find_pk_columns(cols),\n",
    "            })\n",
    "            schema[\"foreign_keys\"].extend(get_foreign_keys(conn, t))\n",
    "\n",
    "        # 3. Sample rows from each table (for JSONL training / debugging)\n",
    "        samples = {t: sample_rows(conn, t, ROWS_PER_TABLE) for t in tables}\n",
    "\n",
    "\n",
    "        # 4. Validation checks (foreign keys, nulls, primary keys)\n",
    "        issues = []\n",
    "        for fk in schema[\"foreign_keys\"]:\n",
    "            from_cols = {c[\"name\"] for c in table_to_columns.get(fk[\"from_table\"], [])}\n",
    "            to_cols   = {c[\"name\"] for c in table_to_columns.get(fk[\"to_table\"], [])}\n",
    "            if fk[\"from_column\"] not in from_cols:\n",
    "                issues.append(f\"Missing from_column {fk['from_table']}.{fk['from_column']}\")\n",
    "            if fk[\"to_column\"] not in to_cols:\n",
    "                issues.append(f\"Missing to_column {fk['to_table']}.{fk['to_column']}\")\n",
    "        null_summary = {t: summarize_nulls(rows) for t, rows in samples.items()}\n",
    "\n",
    "        # Primary key summary for validation.log\n",
    "        pk_summary = {t: [c[\"name\"] for c in cols if c.get(\"is_pk\")]\n",
    "              for t, cols in table_to_columns.items()}\n",
    "        pk_count   = sum(len(pks) for pks in pk_summary.values())\n",
    "\n",
    "\n",
    "        # 5. Save schema.json\n",
    "        with open(schema_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(schema, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "        # 6. Save samples.jsonl\n",
    "        with open(samples_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for t, rows in samples.items():\n",
    "                for r in rows:\n",
    "                    f.write(json.dumps({\"table\": t, \"row\": r}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # 7. Save validation.log\n",
    "        with open(val_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"DB ID: {db_id}\\n\")\n",
    "            f.write(f\"Run:   {RUN_STAMP}\\n\\n\")\n",
    "            f.write(f\"Tables: {len(tables)}\\n\")\n",
    "            f.write(f\"Columns: {sum(len(c) for c in table_to_columns.values())}\\n\")\n",
    "            f.write(f\"PKs: {pk_count}\\n\") \n",
    "            f.write(f\"FKs: {len(schema['foreign_keys'])}\\n\\n\")\n",
    "\n",
    "            # Check for structural issues\n",
    "            if issues:\n",
    "                f.write(\"issues:\\n\")\n",
    "                for i in issues: f.write(f\"- {i}\\n\")\n",
    "            else:\n",
    "                f.write(\"No structural issues found.\\n\")\n",
    "\n",
    "            # Primary key summary\n",
    "            f.write(\"\\nPrimary keys by table:\\n\")\n",
    "            for t, pks in pk_summary.items():\n",
    "                pks = pk_summary[t]\n",
    "                f.write(f\"- {t}: {pks if pks else 'None'}\\n\")\n",
    "\n",
    "            # Null summary\n",
    "            f.write(\"\\nNull summary:\\n\")\n",
    "            f.write(json.dumps(null_summary, indent=2, ensure_ascii=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        # Close connection aggressively to release Windows file locks\n",
    "        try:\n",
    "            if conn is not None:\n",
    "                conn.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "700033f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final orchestration\n",
    "def main():\n",
    "    print(f\"Dataset name:    {DATASET_NAME}\")\n",
    "    print(f\"Dataset dir:     {DATASET_DIR}\")\n",
    "    print(f\"DB root:       {DB_ROOT}\")\n",
    "    print(f\"Artifacts dir: {ARTIFACTS_ROOT}\")\n",
    "\n",
    "    # Safety check: dataset folder must exist \n",
    "    if not DATASET_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Dataset dir {DATASET_DIR} does not exist.\")\n",
    "\n",
    "    # Create artifacts folder if it doesn't exist\n",
    "    ARTIFACTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    dbs = enumerate_sqlite_dbs(DB_ROOT)\n",
    "    if not dbs:\n",
    "        raise RuntimeError(f\"No SQLite DBs found under {DB_ROOT}\")\n",
    "\n",
    "    # Optionally limit the number of databases processed\n",
    "    if MAX_DATABASES and MAX_DATABASES > 0:\n",
    "        # db_id = identifier (usually folder name)\n",
    "        # db_path = actual .sqlite file path\n",
    "        dbs = dbs[:MAX_DATABASES]\n",
    "\n",
    "    # Process each database one by one alphabetically\n",
    "    for db_id, db_path in dbs:\n",
    "        process_one_db(db_id, db_path, ARTIFACTS_ROOT)\n",
    "\n",
    "    print(\"\\ndone.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ec0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name:    spider\n",
      "Dataset dir:     C:\\Users\\felix\\AppData\\Local\\Temp\\spider_6ygax40k\\spider\n",
      "DB root:       C:\\Users\\felix\\AppData\\Local\\Temp\\spider_6ygax40k\\spider\\database\n",
      "Artifacts dir: C:\\Users\\felix\\Videos\\Documents\\GitHub\\project_rdmstokgs\\artifacts\\runs\\spider\n",
      "\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    finally:\n",
    "        # Optional cleanup for temp extraction \n",
    "        if CLEANUP_AFTER_RUN and TMP_DIR is not None:\n",
    "            shutil.rmtree(TMP_DIR, ignore_errors=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
